{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asrithareddy19/hate_speech_detection/blob/main/Hate_speech_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzYOOzY2pDAc"
      },
      "source": [
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVIavB4eo9NI"
      },
      "outputs": [],
      "source": [
        "#defines a neural network model for hate speech detection using the BERT\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import BertForTokenClassification, BertForSequenceClassification,BertPreTrainedModel, BertModel\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "class Model_Rational_Label(BertPreTrainedModel):\n",
        "     def __init__(self,config):\n",
        "        super().__init__(config)\n",
        "        #### Keep this parameters fixed\n",
        "        self.num_labels=2\n",
        "        self.impact_factor=10\n",
        "        ####\n",
        "        self.bert = BertModel(config,add_pooling_layer=False)\n",
        "        self.bert_pooler=BertPooler(config)\n",
        "        self.token_dropout = nn.Dropout(0.1)\n",
        "        self.token_classifier = nn.Linear(config.hidden_size, 2)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
        "        self.init_weights()\n",
        "\n",
        "     def forward(self, input_ids=None, mask=None, attn=None, labels=None):\n",
        "        outputs = self.bert(input_ids, mask)\n",
        "        out=outputs[0]\n",
        "        logits = self.token_classifier(self.token_dropout(out))\n",
        "        embed=self.bert_pooler(outputs[0])\n",
        "        y_pred = self.classifier(self.dropout(embed))\n",
        "        loss_token = None\n",
        "        loss_label = None\n",
        "        loss_total = None\n",
        "\n",
        "        if attn is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if mask is not None:\n",
        "                active_loss = mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, 2)\n",
        "                active_labels = torch.where(\n",
        "                    active_loss, attn.view(-1), torch.tensor(loss_fct.ignore_index).type_as(attn)\n",
        "                )\n",
        "                loss_token = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss_token = loss_fct(logits.view(-1, 2), attn.view(-1))\n",
        "\n",
        "            loss_total=self.impact_factor*loss_token\n",
        "\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_funct = nn.CrossEntropyLoss()\n",
        "            loss_logits =  loss_funct(y_pred.view(-1, self.num_labels), labels.view(-1))\n",
        "            loss_label= loss_logits\n",
        "            if(loss_total is not None):\n",
        "                loss_total+=loss_label\n",
        "            else:\n",
        "                loss_total=loss_label\n",
        "        if(loss_total is not None):\n",
        "            return y_pred, logits, loss_total\n",
        "        else:\n",
        "            return y_pred, logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81wYrHokpsFi"
      },
      "source": [
        "utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFcvZXqppttS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "def softmax(x):    #calculates probabilities\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    temp=e_x / e_x.sum(axis=0) # only difference\n",
        "\n",
        "    if np.isnan(temp).any()==True:\n",
        "        return [0.0,1.0,0.0]\n",
        "    else:\n",
        "        return temp\n",
        "class CharVal(object):\n",
        "    def __init__(self, char, val):\n",
        "        self.char = char\n",
        "        self.val = val\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.char\n",
        "\n",
        "def rgb_to_hex(rgb):\n",
        "    return '#%02x%02x%02x' % rgb\n",
        "\n",
        "\n",
        "def color_charvals_lime(s):\n",
        "    r = 255-int(s.val*255)\n",
        "    color = rgb_to_hex((255, r, r))\n",
        "    return 'background-color: %s' % color\n",
        "def color_charvals_rationale(s):\n",
        "    r = 255-int(s.val*255)\n",
        "    color = rgb_to_hex((255, r, r))\n",
        "    return 'background-color: %s' % color\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN-a7w_jpGdj"
      },
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toVz9kuKqAZJ",
        "outputId": "ff8b84e2-b10b-4b09-b47c-161e03974287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ekphrasis\n",
            "  Downloading ekphrasis-0.5.4-py3-none-any.whl (83 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/83.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (2.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (4.66.2)\n",
            "Collecting colorama (from ekphrasis)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting ujson (from ekphrasis)\n",
            "  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (3.7.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (3.8.1)\n",
            "Collecting ftfy (from ekphrasis)\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (1.25.2)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->ekphrasis) (0.2.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (2023.12.25)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->ekphrasis) (1.16.0)\n",
            "Installing collected packages: ujson, ftfy, colorama, ekphrasis\n",
            "Successfully installed colorama-0.4.6 ekphrasis-0.5.4 ftfy-6.2.0 ujson-5.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install ekphrasis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgBRZUeCpIdg",
        "outputId": "08613c48-5f09-4b10-c56c-b468ee685916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "Reading twitter - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading english - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/english/counts_1grams.txt\n"
          ]
        }
      ],
      "source": [
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "import re\n",
        "from transformers import AutoTokenizer,AutoModelForSequenceClassification,AutoConfig\n",
        "import numpy as np\n",
        "import torch\n",
        "#fixing up typos, removing website links, also identifies and tags things like hashtags and repeated words\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'date', 'number'],\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    segmenter=\"twitter\",\n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "\n",
        "# this class encapsulates the functionality to preprocess input text, tokenize it,\n",
        "#perform model inference, and extract attention vectors, providing a streamlined approach to predict rational labels for a given set of sentence\n",
        "class modelPredRationale():\n",
        "    def __init__(self, model_path = 'bert-base-uncased', device = None):\n",
        "        self.device = device\n",
        "        self.model_path=model_path\n",
        "        self.model = Model_Rational_Label.from_pretrained(model_path,output_attentions = True,output_hidden_states = False).to(self.device)\n",
        "        self.config = AutoConfig.from_pretrained(self.model_path)\n",
        "        if torch.cuda.is_available():\n",
        "            self.model.cuda()\n",
        "        self.model.eval()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, use_fast = False)\n",
        "\n",
        "    def preprocess_func(self, text):\n",
        "        remove_words=['<allcaps>','</allcaps>','<hashtag>','</hashtag>','<elongated>','<emphasis>','<repeated>','\\'','s']\n",
        "        word_list=text_processor.pre_process_doc(text)\n",
        "        word_list=list(filter(lambda a: a not in remove_words, word_list))\n",
        "        sent=\" \".join(word_list)\n",
        "        sent = re.sub(r\"[<\\*>]\", \" \",sent)\n",
        "        return sent\n",
        "\n",
        "    def tokenize(self, sentences, padding = True, max_len = 128):\n",
        "        input_ids, attention_masks, token_type_ids = [], [], []\n",
        "        for sent in sentences:\n",
        "            encoded_dict = self.tokenizer.encode_plus(sent,\n",
        "                                                    add_special_tokens=True,\n",
        "                                                    max_length=max_len,\n",
        "                                                    padding='max_length',\n",
        "                                                    return_attention_mask = True,\n",
        "                                                    return_tensors = 'pt',\n",
        "                                                    truncation = True)\n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "        input_ids = torch.cat(input_ids, dim=0)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "        return {'input_ids': input_ids, 'attention_masks': attention_masks}\n",
        "\n",
        "    def process_data(self, sentences_list):\n",
        "        sentences = []\n",
        "        sentence_lengths = []\n",
        "        for sentence in sentences_list:\n",
        "            try:\n",
        "                sentence = self.preprocess_func(sentence)\n",
        "            except TypeError:\n",
        "                sentence = self.preprocess_func(\"dummy text\")\n",
        "            sentences.append(sentence)\n",
        "            sentence_lengths.append(len(self.tokenizer.encode(sentence)))\n",
        "        inputs = self.tokenize(sentences)\n",
        "        tokenized_sentences = [self.tokenizer.convert_ids_to_tokens(ele) for ele in  inputs['input_ids']]\n",
        "\n",
        "        return self.get_dataloader(inputs), sentence_lengths, tokenized_sentences\n",
        "\n",
        "    def get_dataloader(self, inputs):\n",
        "        data = TensorDataset(inputs['input_ids'], inputs['attention_masks'])\n",
        "        sampler = SequentialSampler(data)\n",
        "        return DataLoader(data, sampler=sampler, batch_size=32)\n",
        "\n",
        "\n",
        "    def return_rationales(self, sentences_list):\n",
        "        \"\"\"Input: should be a list of sentences\"\"\"\n",
        "        \"\"\"Output: probablity values\"\"\"\n",
        "        device = self.device\n",
        "\n",
        "        test_dataloader,sentence_lengths, tokenized_sentences=self.process_data(sentences_list)\n",
        "\n",
        "        print(\"Running eval on test data...\")\n",
        "        labels_list=[]\n",
        "        rationale_list=[]\n",
        "        rationale_logit_list = []\n",
        "        # Evaluate data\n",
        "        for step,batch in enumerate(test_dataloader):\n",
        "\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "\n",
        "            label_logits, rationale_logits = self.model(b_input_ids, b_input_mask)\n",
        "\n",
        "            label_logits = label_logits.detach().cpu().numpy()\n",
        "            rationale_logits = rationale_logits.detach().cpu().numpy()\n",
        "\n",
        "            final_logits=[]\n",
        "            final_rationales=[]\n",
        "            for i in range(label_logits.shape[0]):\n",
        "                final_logits.append(softmax(label_logits[i]))\n",
        "                final_rationales.append([ele[1] for ele in rationale_logits[i]])\n",
        "            labels_list+=final_logits\n",
        "            rationale_list+=final_rationales\n",
        "\n",
        "        attention_vectors = []\n",
        "        for idx, rationales in enumerate(rationale_list):\n",
        "            attention_vector = softmax(rationales[:sentence_lengths[idx]])\n",
        "            attention_vector = list(attention_vector) + [0]*(128-len(list(attention_vector)))\n",
        "            attention_vectors.append(attention_vector)\n",
        "\n",
        "        tokens_sentence=[]\n",
        "        for idx, tokenized in enumerate(tokenized_sentences):\n",
        "            tokenized = tokenized[:sentence_lengths[idx]]\n",
        "            tokens_sentence.append(tokenized)\n",
        "\n",
        "        return np.array(labels_list), np.array(attention_vectors), tokens_sentence\n",
        "\n",
        "\n",
        "#this class encapsulates the functionality required to preprocess text,\n",
        "#tokenize it, pass it through a pre-trained language model, and obtain predictions for text classification tasks.\n",
        "class modelPred():\n",
        "    def __init__(self, language='english', device=None):\n",
        "        self.__modelDict ={\n",
        "        'arabic':\"Hate-speech-CNERG/dehatebert-mono-arabic\",\n",
        "        'english': \"Hate-speech-CNERG/dehatebert-mono-english\",\n",
        "        'english_hatexplain':\"Hate-speech-CNERG/bert-base-uncased-hatexplain\",\n",
        "        'french': \"Hate-speech-CNERG/dehatebert-mono-french\",\n",
        "        'german': \"Hate-speech-CNERG/dehatebert-mono-german\",\n",
        "        'indonesian': \"Hate-speech-CNERG/dehatebert-mono-indonesian\",\n",
        "        'polish': \"Hate-speech-CNERG/dehatebert-mono-polish\",\n",
        "        'portugese': \"Hate-speech-CNERG/dehatebert-mono-portugese\",\n",
        "        'italian': \"Hate-speech-CNERG/dehatebert-mono-italian\",\n",
        "        'spanish': \"Hate-speech-CNERG/dehatebert-mono-spanish\"\n",
        "        }\n",
        "        self.device = device\n",
        "        self.model_path=self.__modelDict[language]\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
        "        self.config = AutoConfig.from_pretrained(self.model_path)\n",
        "        # if(model_name=='xlmr'):\n",
        "        #     self.model = XLMRobertaForSequenceClassification.from_pretrained(self.model_path,output_attentions = True,output_hidden_states = False).to(self.device)\n",
        "        # elif(model_name=='bert'):\n",
        "        #     self.model = BertForSequenceClassification.from_pretrained(self.model_path,output_attentions = True,output_hidden_states = False).to(self.device)\n",
        "        self.model.cuda()\n",
        "        self.model.eval()\n",
        "\n",
        "    def preprocess_func(self, text):\n",
        "        new_text = re.sub('@\\w+', '@user',text)\n",
        "        new_text = new_text.replace(\"\\r\\n\\'\",' ').replace(\"\\n\",' ')\n",
        "        new_text = re.sub(r\"http\\S+\", \"\", new_text)\n",
        "        new_text = new_text.replace('&amp;', '&')\n",
        "        return new_text\n",
        "\n",
        "    def tokenize(self, sentences, padding = True, max_len = 128):\n",
        "        input_ids, attention_masks, token_type_ids = [], [], []\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
        "        for sent in sentences:\n",
        "            encoded_dict = self.tokenizer.encode_plus(sent,\n",
        "                                                    add_special_tokens=True,\n",
        "                                                    max_length=max_len,\n",
        "                                                    padding='max_length',\n",
        "                                                    return_attention_mask = True,\n",
        "                                                    return_tensors = 'pt',\n",
        "                                                    truncation = True)\n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "        input_ids = torch.cat(input_ids, dim=0)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "        return {'input_ids': input_ids, 'attention_masks': attention_masks}\n",
        "\n",
        "    def process_data(self, sentences_list):\n",
        "        sentences = []\n",
        "        for sentence in sentences_list:\n",
        "            try:\n",
        "                sentence = self.preprocess_func(sentence)\n",
        "            except TypeError:\n",
        "                sentence = self.preprocess_func(\"dummy text\")\n",
        "            sentences.append(sentence)\n",
        "        inputs = self.tokenize(sentences)\n",
        "        return self.get_dataloader(inputs)\n",
        "\n",
        "    def get_dataloader(self, inputs):\n",
        "        data = TensorDataset(inputs['input_ids'], inputs['attention_masks'])\n",
        "        sampler = SequentialSampler(data)\n",
        "        return DataLoader(data, sampler=sampler, batch_size=32)\n",
        "\n",
        "    def return_probab(self, sentences_list):\n",
        "        \"\"\"Input: should be a list of sentences\"\"\"\n",
        "        \"\"\"Output: probablity values\"\"\"\n",
        "        device = self.device\n",
        "\n",
        "        test_dataloader=self.process_data(sentences_list)\n",
        "\n",
        "        print(\"Running eval on test data...\")\n",
        "        labels_list=[]\n",
        "        sentence_lengths = [len(self.tokenizer.encode(sentence)) for sentence in  sentences_list]\n",
        "        # Evaluate data\n",
        "        for step,batch in enumerate(test_dataloader):\n",
        "\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "\n",
        "            label_logits = self.model(b_input_ids, b_input_mask).logits\n",
        "            label_logits = label_logits.detach().cpu().numpy()\n",
        "\n",
        "            final_logits=[]\n",
        "            for i in range(label_logits.shape[0]):\n",
        "                final_logits.append(softmax(label_logits[i]))\n",
        "            labels_list+=final_logits\n",
        "\n",
        "        return np.array(labels_list)\n",
        "\n",
        "# modelPredRationale is tailored for models providing explanations,\n",
        "#while modelPred is a more general-purpose class for making predictions with language models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTB7UXsm3_mF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import transformers\n",
        "import random\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import BertForSequenceClassification\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import re\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZbRedQYZKZ1"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "   device = torch.device(\"cuda\")\n",
        "else:\n",
        "   device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWgKwG1oVUQF"
      },
      "outputs": [],
      "source": [
        "def getDatasetPrediction(dataset,config):\n",
        "    labels=model.return_probab(dataset['Sentences'])\n",
        "    predictions = {}\n",
        "    for index, row in dataset.iterrows():\n",
        "\n",
        "        dict1={}\n",
        "        dict1['Sentence']=row['Sentences']\n",
        "        dict_labels={}\n",
        "        for ele in config:\n",
        "            dict_labels[config[ele]]=round(labels[index][ele],3)\n",
        "        dict1[\"Labels\"]=dict_labels\n",
        "        predictions[row['Index']] = dict1\n",
        "    return predictions\n",
        "\n",
        "def getRandomTextFromPred(pred = None):\n",
        "    return random.choice(list(prediction.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBsVw6f7m0Vv"
      },
      "outputs": [],
      "source": [
        "#@title ### **Select a language**\n",
        "Language = \"Arabic\" #@param [\"Arabic\", \"English\", \"French\", \"German\", \"Indonesian\", \"Polish\", \"Portugese\", \"Italian\", \"Spanish\", \"English_hatexplain\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rFPd30qXYVi"
      },
      "outputs": [],
      "source": [
        "model = modelPred(language=Language.lower(), device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh217P4G1Ksc"
      },
      "outputs": [],
      "source": [
        "#@title **How do you want to enter text ?**\n",
        "# @markdown You can either directly enter the text (text input) or uppload from a csv (file)\n",
        "input_type = \"text input\" #@param [\"file\", \"text input\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkFxfc1IZl4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7100c5cf-4bb5-456c-8532-669baf18ebd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Write the post:  قطر مرادف الارهاب يا حمار قمة يعني زبالة طيب كلي هوا وخليكي بزباله لبنان حسن عليق؟؟؟ كول هوا ابوك الحرامي طبعا يا عيلة وسخة بلا شرف ولا اخلاق\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "if input_type == \"text input\":\n",
        "    text_input = input(\"Write the post: \")\n",
        "    dataset=[text_input]\n",
        "else:\n",
        "  print(\"Please upload the csv file you want to get predictions\")\n",
        "  print(\"Please make sure the column name of the csv should be Index, Sentences\")\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()\n",
        "  dataset = pd.read_csv(io.BytesIO(uploaded[list(uploaded)[0]])).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1rQS0GzT8nn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14a5f1a9-3105-4424-c303-2aee1c051b14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running eval on test data...\n",
            "{'Sentence': ' قطر مرادف الارهاب يا حمار قمة يعني زبالة طيب كلي هوا وخليكي بزباله لبنان حسن عليق؟؟؟ كول هوا ابوك الحرامي طبعا يا عيلة وسخة بلا شرف ولا اخلاق', 'Labels': {'NON_HATE': 0.025, 'HATE': 0.975}}\n"
          ]
        }
      ],
      "source": [
        "if input_type == \"text input\":\n",
        "  labels=model.return_probab(dataset)\n",
        "  dict1={}\n",
        "  dict1['Sentence']=dataset[0]\n",
        "  dict_labels={}\n",
        "  config=model.config.id2label\n",
        "  for ele in config:\n",
        "      dict_labels[config[ele]]=round(labels[0][ele],3)\n",
        "  dict1[\"Labels\"]=dict_labels\n",
        "  print(dict1)\n",
        "else:\n",
        "  prediction = getDatasetPrediction(dataset,model.config.id2label)\n",
        "  print(getRandomTextFromPred(prediction))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA9lZ_x1BrPe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "outputId": "b491d393-d91b-4e7d-a37e-1a05021da237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.2.2)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2024.4.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17395 sha256=ef8f568d9185ba5d7465927ae250c89c7471585615bd7292dd636e075815d87f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.4.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "chardet",
                  "idna"
                ]
              },
              "id": "694350b75b5444aa82ec03c76a6b8392"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install googletrans==4.0.0-rc1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDp0pcsdBtGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d55531-9c9a-4e84-92fa-db75b3868aa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated text: Qatar is the synonym of terrorism, O donkey, a summit, I mean, the garbage is good, Hawa, and I will leave you, Lebanon, Hassan Alik ???Cole Hua, your father, the thief, of course, a dirty family, without honor or morals\n"
          ]
        }
      ],
      "source": [
        "from googletrans import Translator\n",
        "\n",
        "def translate(text, source_lang='auto', dest_lang='en'):\n",
        "    translator = Translator()\n",
        "    translated_text = translator.translate(text, src=source_lang, dest=dest_lang)\n",
        "    return translated_text.text\n",
        "\n",
        "text_to_translate = text_input\n",
        "\n",
        "translated_text = translate(text_to_translate, dest_lang='en')\n",
        "print(\"Translated text:\", translated_text)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}